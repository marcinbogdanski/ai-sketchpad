{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)  # init TF ...\n",
    "config=tf.ConfigProto(gpu_options=gpu_options)  # w/o taking ...\n",
    "with tf.Session(config=config): pass            # all GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dates_raw = []\n",
    "human_vocab = set()\n",
    "machine_dates_raw = []\n",
    "machine_vocab = set()\n",
    "with open(os.path.expanduser('~/dates_train.csv'), 'r') as f:\n",
    "    for line in f:\n",
    "        h, m = line.strip().split(',')\n",
    "        human_dates_raw.append(h)\n",
    "        human_vocab.update(h)\n",
    "        machine_dates_raw.append(m)\n",
    "        machine_vocab.update(m)\n",
    "        \n",
    "#human_vocab = ['<pad>', *sorted(human_vocab)]\n",
    "#machine_vocab = sorted(machine_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9 may 1998', '10.09.70', '4/28/90', 'thursday january 26 1995']\n"
     ]
    }
   ],
   "source": [
    "print(human_dates_raw[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1998-05-09', '1970-09-10', '1990-04-28', '1995-01-26']\n"
     ]
    }
   ],
   "source": [
    "print(machine_dates_raw[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_vocab:   [' ', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n",
      "machine_vocab: ['-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "print('human_vocab:  ', sorted(human_vocab))\n",
    "print('machine_vocab:', sorted(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '.': 2, '/': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, 'a': 14, 'b': 15, 'c': 16, 'd': 17, 'e': 18, 'f': 19, 'g': 20, 'h': 21, 'i': 22, 'j': 23, 'l': 24, 'm': 25, 'n': 26, 'o': 27, 'p': 28, 'r': 29, 's': 30, 't': 31, 'u': 32, 'v': 33, 'w': 34, 'y': 35}\n"
     ]
    }
   ],
   "source": [
    "human_sym2idx = {c : i for i, c in enumerate(sorted(human_vocab), 1)}  # 0 is for padding\n",
    "print(human_sym2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine_sym2idx: {'-': 0, '0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10}\n",
      "machine_idx2sym: {0: '-', 1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9'}\n"
     ]
    }
   ],
   "source": [
    "machine_sym2idx = {c : i for i, c in enumerate(sorted(machine_vocab))}\n",
    "machine_idx2sym = {i : c for c, i in machine_sym2idx.items()}\n",
    "print('machine_sym2idx:', machine_sym2idx)\n",
    "print('machine_idx2sym:', machine_idx2sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human dates min-max len: 6 - 27\n"
     ]
    }
   ],
   "source": [
    "human_min_len = len(min(human_dates_raw, key=len))\n",
    "human_max_len = len(max(human_dates_raw, key=len))\n",
    "machine_min_len = len(min(machine_dates_raw, key=len))\n",
    "machine_max_len = len(max(machine_dates_raw, key=len))\n",
    "assert machine_min_len == machine_max_len == 10  # sanity check\n",
    "print('Human dates min-max len:', human_min_len, '-', human_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(list_of_examples, sym2idx):\n",
    "    result_tokenized = []\n",
    "    for list_of_symbols in list_of_examples:\n",
    "        result_tokenized.append( [sym2idx[s] for s in list_of_symbols] )\n",
    "    return result_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 1, 25, 14, 35, 1, 5, 13, 13, 12],\n",
       " [5, 4, 2, 4, 13, 2, 11, 4],\n",
       " [8, 3, 6, 12, 3, 13, 4]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_dates_tok = tokenize(human_dates_raw, human_sym2idx)\n",
    "display(human_dates_tok[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 10, 10, 9, 0, 1, 6, 0, 1, 10],\n",
       " [2, 10, 8, 1, 0, 1, 10, 0, 2, 1],\n",
       " [2, 10, 10, 1, 0, 1, 5, 0, 3, 9]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "machine_dates_tok = tokenize(machine_dates_raw, machine_sym2idx)\n",
    "display(machine_dates_tok[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(list_of_examples, target_len, dtype):\n",
    "    result_np = np.zeros(shape=(len(list_of_examples), target_len), dtype=dtype)\n",
    "    \n",
    "    for i in range(len(list_of_examples)):\n",
    "        example = list_of_examples[i]\n",
    "        result_np[i, 0:len(example)] = example\n",
    "        \n",
    "    return result_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10000, 27)\n",
      "[13  1 25 14 35  1  5 13 13 12  0  0  0  0  0  0  0  0  0  0]\n",
      "[ 5  4  2  4 13  2 11  4  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[ 8  3  6 12  3 13  4  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "human_dates_pad = pad(human_dates_tok, human_max_len, dtype=int)\n",
    "print('shape:', human_dates_pad.shape)\n",
    "for i in range(3):\n",
    "    print(human_dates_pad[i][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10000, 10)\n",
      "[ 2 10 10  9  0  1  6  0  1 10]\n",
      "[ 2 10  8  1  0  1 10  0  2  1]\n",
      "[ 2 10 10  1  0  1  5  0  3  9]\n"
     ]
    }
   ],
   "source": [
    "machine_dates_pad = np.array(machine_dates_tok)  # nothing to pad\n",
    "print('shape:', machine_dates_pad.shape)\n",
    "for i in range(3):\n",
    "    print(machine_dates_pad[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_2d(array, nb_class):\n",
    "    assert len(array.shape) == 2\n",
    "    \n",
    "    result = np.zeros((*array.shape, nb_class))\n",
    "    for i in range(len(array)):\n",
    "        for j in range(array.shape[1]):\n",
    "            k = array[i, j]\n",
    "            result[i, j, k] = 1\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_max_idx = max(human_sym2idx.values())\n",
    "human_dates_oh = onehot_2d(human_dates_pad, human_max_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_max_idx = max(machine_sym2idx.values())\n",
    "machine_dates_oh = onehot_2d(machine_dates_pad, machine_max_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date: 1998-05-09\n",
      "padded: [ 2 10 10  9  0  1  6  0  1 10]\n",
      "onehot:\n",
      " [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print('date:', machine_dates_raw[0])\n",
    "print('padded:', machine_dates_pad[0])\n",
    "print('onehot:\\n', machine_dates_oh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_hseq 27\n",
      "n_hout 36\n",
      "n_mseq 10\n",
      "n_mout 11\n"
     ]
    }
   ],
   "source": [
    "n_hseq = human_max_len\n",
    "n_hout = human_max_idx + 1\n",
    "n_mseq = machine_max_len\n",
    "n_mout = machine_max_idx + 1\n",
    "print('n_hseq', n_hseq)\n",
    "print('n_hout', n_hout)\n",
    "print('n_mseq', n_mseq)\n",
    "print('n_mout', n_mout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def softmax(x, axis=1):\n",
    "    \"\"\"Softmax activation function.\n",
    "    # Arguments\n",
    "        x : Tensor.\n",
    "        axis: Integer, axis along which the softmax normalization is applied.\n",
    "    # Returns\n",
    "        Tensor, output of softmax transformation.\n",
    "    # Raises\n",
    "        ValueError: In case `dim(x) == 1`.\n",
    "    \"\"\"\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = tf.keras.layers.RepeatVector(n_hseq)\n",
    "concatenator = tf.keras.layers.Concatenate(axis=-1)\n",
    "densor1 = tf.keras.layers.Dense(10, activation = \"tanh\")\n",
    "densor2 = tf.keras.layers.Dense(1, activation = None) # \"relu\")\n",
    "activator = tf.keras.layers.Activation(softmax, name='attention_weights') # custom softmax\n",
    "dotor = tf.keras.layers.Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_step_attention\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # a (?, 27, 2*32)\n",
    "    # s_prev (?, 64)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)  # (?, 27, 64)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a, s_prev])  # (?, 27, 128)\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
    "    e = densor1(concat)  # (?, 27, 10)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
    "    energies = densor2(e)  # (?, 27, 1)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(energies)  # (?, 27, 1)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas, a])  # (?, 1, 64)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 32\n",
    "n_s = 64\n",
    "post_activation_LSTM_cell = tf.keras.layers.LSTM(n_s, return_state = True)\n",
    "output_layer = tf.keras.layers.Dense(n_mout, activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def create_model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # pdb.set_trace()\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = tf.keras.Input(shape=(Tx, human_vocab_size))  # (?, 27, 36)\n",
    "    s0 = tf.keras.Input(shape=(n_s,), name='s0')      # (?, 64)\n",
    "    c0 = tf.keras.Input(shape=(n_s,), name='c0')      # (?, 64)\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    attentions = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(n_a, return_sequences=True))(X)  # (?, 27, 2*32)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context, alphas = one_step_attention(a, s)  # (?, 1, 64)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c]) # (?, 1, 64), (?, 1, 64), (?, 1, 64)\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)  # (?, 11)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "        attentions.append(alphas)\n",
    "        \n",
    "    outputs2 = tf.keras.layers.Concatenate()(outputs)\n",
    "    outputs3 = tf.keras.layers.Reshape((Ty, machine_vocab_size))(outputs2)\n",
    "    \n",
    "    attentions2 = tf.keras.layers.Concatenate()(attentions)\n",
    "    attentions3 = tf.keras.layers.Permute((2, 1))(attentions2)\n",
    "    \n",
    "    # pdb.set_trace()\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model_train = tf.keras.Model([X, s0, c0], outputs3)\n",
    "    model_attend = tf.keras.Model([X, s0, c0], [outputs3, attentions3])\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model_train, model_attend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marcin/.anaconda/envs/tfgpu113/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model, model_attend = create_model(n_hseq, n_mseq, n_a, n_s, n_hout, n_mout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 27, 36)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 27, 64)       17664       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 27, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[8][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 27, 128)      0           bidirectional[0][0]              \n",
      "                                                                 repeat_vector[0][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[1][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[2][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[3][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[4][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[5][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[6][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[7][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[8][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 27, 10)       1290        concatenate[0][0]                \n",
      "                                                                 concatenate[1][0]                \n",
      "                                                                 concatenate[2][0]                \n",
      "                                                                 concatenate[3][0]                \n",
      "                                                                 concatenate[4][0]                \n",
      "                                                                 concatenate[5][0]                \n",
      "                                                                 concatenate[6][0]                \n",
      "                                                                 concatenate[7][0]                \n",
      "                                                                 concatenate[8][0]                \n",
      "                                                                 concatenate[9][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 27, 1)        11          dense[0][0]                      \n",
      "                                                                 dense[1][0]                      \n",
      "                                                                 dense[2][0]                      \n",
      "                                                                 dense[3][0]                      \n",
      "                                                                 dense[4][0]                      \n",
      "                                                                 dense[5][0]                      \n",
      "                                                                 dense[6][0]                      \n",
      "                                                                 dense[7][0]                      \n",
      "                                                                 dense[8][0]                      \n",
      "                                                                 dense[9][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 27, 1)        0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 64), (None,  33024       dot[0][0]                        \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot[1][0]                        \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "                                                                 dot[2][0]                        \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[1][2]                       \n",
      "                                                                 dot[3][0]                        \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[2][2]                       \n",
      "                                                                 dot[4][0]                        \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[3][2]                       \n",
      "                                                                 dot[5][0]                        \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[4][2]                       \n",
      "                                                                 dot[6][0]                        \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[5][2]                       \n",
      "                                                                 dot[7][0]                        \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[6][2]                       \n",
      "                                                                 dot[8][0]                        \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[7][2]                       \n",
      "                                                                 dot[9][0]                        \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 lstm[8][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 11)           715         lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 lstm[9][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 110)          0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "                                                                 dense_2[6][0]                    \n",
      "                                                                 dense_2[7][0]                    \n",
      "                                                                 dense_2[8][0]                    \n",
      "                                                                 dense_2[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 10, 11)       0           concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 52,704\n",
      "Trainable params: 52,704\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈2 lines)\n",
    "opt = tf.keras.optimizers.Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(human_dates_oh)\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "#outputs = list(machine_dates_oh.swapaxes(0,1))\n",
    "outputs = machine_dates_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target Output**\n",
    "\n",
    "Epoch xx/xx\n",
    "10000/10000 [==============================] - 15s 2ms/step - loss: 0.0543 - dense_3_loss: 0.0021 - dense_3_acc: 0.9998 - dense_3_acc_1: 1.0000 - dense_3_acc_2: 1.0000 - dense_3_acc_3: 1.0000 - dense_3_acc_4: 1.0000 - dense_3_acc_5: 0.9982 - dense_3_acc_6: 0.9997 - dense_3_acc_7: 1.0000 - dense_3_acc_8: 0.9958 - dense_3_acc_9: 0.9997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marcin/.anaconda/envs/tfgpu113/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/marcin/.anaconda/envs/tfgpu113/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 1.6838 - acc: 0.4080\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.9342 - acc: 0.6462\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.7691 - acc: 0.7166\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.6307 - acc: 0.7763\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.5041 - acc: 0.8312\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.4136 - acc: 0.8660\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.3409 - acc: 0.8938\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.2878 - acc: 0.9135\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.2464 - acc: 0.9269\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.2170 - acc: 0.9364\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.1909 - acc: 0.9451\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.1708 - acc: 0.9509\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.1540 - acc: 0.9560\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.1398 - acc: 0.9606\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.1273 - acc: 0.9649\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.1169 - acc: 0.9676\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.1077 - acc: 0.9704\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0995 - acc: 0.9728\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0928 - acc: 0.9746\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0869 - acc: 0.9761\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0821 - acc: 0.9776\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0772 - acc: 0.9790\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0731 - acc: 0.9807\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0689 - acc: 0.9818\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0656 - acc: 0.9827\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0624 - acc: 0.9839\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0596 - acc: 0.9847\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0566 - acc: 0.9859\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0543 - acc: 0.9864\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0516 - acc: 0.9876\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0494 - acc: 0.9883\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0477 - acc: 0.9886\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0454 - acc: 0.9898\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0433 - acc: 0.9901\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0417 - acc: 0.9907\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0398 - acc: 0.9914\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0385 - acc: 0.9916\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0370 - acc: 0.9920\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0355 - acc: 0.9924\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0343 - acc: 0.9928\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0331 - acc: 0.9930\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0318 - acc: 0.9934\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0306 - acc: 0.9936\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0297 - acc: 0.9938\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0283 - acc: 0.9942\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0275 - acc: 0.9944\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0267 - acc: 0.9947\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0257 - acc: 0.9949\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0251 - acc: 0.9948\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0240 - acc: 0.9952\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([human_dates_oh, s0, c0], outputs, epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('2410_attention_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('2410_attention_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-03\n",
      "correc: 1979-05-03\n",
      "is ok:  YES\n",
      "\n",
      "source: 5 April 09\n",
      "output: 2009-04-05\n",
      "correc: 2009-04-05\n",
      "is ok:  YES\n",
      "\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-11\n",
      "correc: 2016-08-21\n",
      "is ok:  no\n",
      "\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10\n",
      "correc: 2007-07-10\n",
      "is ok:  YES\n",
      "\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09\n",
      "correc: 2018-05-09\n",
      "is ok:  YES\n",
      "\n",
      "source: March 3 2001\n",
      "output: 2001-03-03\n",
      "correc: 2001-03-03\n",
      "is ok:  YES\n",
      "\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03\n",
      "correc: 2001-03-03\n",
      "is ok:  YES\n",
      "\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01\n",
      "correc: 2001-03-01\n",
      "is ok:  YES\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "CORRECT =  ['1979-05-03', '2009-04-05', '2016-08-21',          '2007-07-10',      '2018-05-09', '2001-03-03', '2001-03-03', '2001-03-01']\n",
    "for example, correct in zip(EXAMPLES, CORRECT):\n",
    "    \n",
    "    source = onehot_2d(pad(tokenize([example.lower()], human_sym2idx), n_hseq, int), n_hout)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis=-1)\n",
    "    output = [machine_idx2sym[i] for i in prediction.ravel()]\n",
    "    output = ''.join(output)\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", output)\n",
    "    print('correc:', correct)\n",
    "    print('is ok: ', 'YES' if output==correct else 'no')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['acc'])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_map = plot_attention_map(model_attend, human_sym2idx, machine_idx2sym, \"Tuesday 09 Oct 1993\", num=7, n_s = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dates_valid_raw = []\n",
    "machine_dates_valid_raw = []\n",
    "with open(os.path.expanduser('~/dates_valid.csv'), 'r') as f:\n",
    "    for line in f:\n",
    "        h, m = line.strip().split(',')\n",
    "        human_dates_valid_raw.append(h)\n",
    "        machine_dates_valid_raw.append(m)\n",
    "        \n",
    "#human_vocab = ['<pad>', *sorted(human_vocab)]\n",
    "#machine_vocab = sorted(machine_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_correct 948\n"
     ]
    }
   ],
   "source": [
    "count_corret = 0\n",
    "for i in range(1000):\n",
    "#for example, correct in zip(human_dates_valid_raw, machine_dates_valid_raw):\n",
    "    \n",
    "    example = human_dates_valid_raw[i]\n",
    "    correct = machine_dates_valid_raw[i]\n",
    "    \n",
    "    source = onehot_2d(pad(tokenize([example.lower()], human_sym2idx), n_hseq, int), n_hout)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis=-1)\n",
    "    output = [machine_idx2sym[i] for i in prediction.ravel()]\n",
    "    output = ''.join(output)\n",
    "    \n",
    "#     print(\"source:\", example)\n",
    "#     print(\"output:\", output)\n",
    "#     print('correc:', correct)\n",
    "#     print('is ok: ', 'YES' if output==correct else 'no')\n",
    "#     print()\n",
    "    \n",
    "    if output==correct:\n",
    "        count_corret += 1\n",
    "        \n",
    "print('count_correct', count_corret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_hseq 27\n",
      "n_hout 36\n",
      "n_mseq 10\n",
      "n_mout 11\n"
     ]
    }
   ],
   "source": [
    "print('n_hseq', n_hseq)\n",
    "print('n_hout', n_hout)\n",
    "print('n_mseq', n_mseq)\n",
    "print('n_mout', n_mout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_map(model, input_vocabulary, inv_output_vocabulary, text, n_s = 128, num = 6, Tx = 30, Ty = 10):\n",
    "    \"\"\"\n",
    "    Plot the attention map.\n",
    "  \n",
    "    \"\"\"\n",
    "    attention_mapA = np.zeros((n_mseq, n_hseq))\n",
    "    Ty, Tx = attention_mapA.shape\n",
    "    print('Ty, Tx', attention_mapA.shape)\n",
    "    \n",
    "    s0 = np.zeros((1, n_s))\n",
    "    c0 = np.zeros((1, n_s))\n",
    "#     layer = model.layers[num]\n",
    "\n",
    "    encoded = onehot_2d(pad(tokenize([text.lower()], human_sym2idx), n_hseq, int), n_hout)\n",
    "    encoded = encoded.astype(np.float32)\n",
    "    \n",
    "    print(Ty)\n",
    "    print(encoded.shape, encoded.dtype)\n",
    "    print(s0.shape, s0.dtype)\n",
    "    print(c0.shape, c0.dtype)\n",
    "\n",
    "    \n",
    "#     pdb.set_trace()\n",
    "\n",
    "#     f = K.function(model.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
    "#     r = f([encoded, s0, c0])\n",
    "    \n",
    "#     for t in range(Ty):\n",
    "#         for t_prime in range(Tx):\n",
    "#             attention_map[t][t_prime] = r[t][0,t_prime,0]\n",
    "\n",
    "\n",
    "\n",
    "    prediction, attention_map = model.predict([encoded, s0, c0])\n",
    "    attention_map = attention_map.squeeze()\n",
    "    #attention_map = attention_map.T\n",
    "    #print(prediction)\n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    prediction = np.argmax(prediction, axis=-1)\n",
    "    predicted_text = [machine_idx2sym[i] for i in prediction.ravel()]\n",
    "    #print(predicted_text)\n",
    "    \n",
    "#     predicted_text = []\n",
    "#     for i in range(len(prediction)):\n",
    "#         predicted_text.append(int(np.argmax(prediction[i], axis=1)))\n",
    "        \n",
    "#     predicted_text = list(predicted_text)\n",
    "#     print(predicted_text)\n",
    "#     predicted_text = [machine_idx2sym[i] for i in predicted_text.ravel()]\n",
    "    #predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
    "    text_ = list(text)\n",
    "    \n",
    "    # get the lengths of the string\n",
    "    input_length = len(text)\n",
    "    output_length = Ty\n",
    "    \n",
    "    # Plot the attention_map\n",
    "    plt.clf()\n",
    "    f = plt.figure(figsize=(8, 8.5))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "\n",
    "    # add image\n",
    "    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')\n",
    "\n",
    "    # add colorbar\n",
    "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
    "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
    "    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
    "\n",
    "    # add labels\n",
    "    ax.set_yticks(range(output_length))\n",
    "    ax.set_yticklabels(predicted_text[:output_length])\n",
    "\n",
    "    ax.set_xticks(range(input_length))\n",
    "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
    "\n",
    "    ax.set_xlabel('Input Sequence')\n",
    "    ax.set_ylabel('Output Sequence')\n",
    "\n",
    "    # add grid and legend\n",
    "    ax.grid()\n",
    "\n",
    "    #f.show()\n",
    "    \n",
    "    return attention_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "attentions2, attention_map = plot_attention_map(model_attend, human_sym2idx, machine_idx2sym, \"Tuesday 09 Oct 1993\", num=7, n_s = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfgpu113]",
   "language": "python",
   "name": "conda-env-tfgpu113-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
